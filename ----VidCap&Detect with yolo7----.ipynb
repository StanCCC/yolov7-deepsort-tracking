{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "We have 3 important files for this purpose and each and every dependency, class, import, function, variable etc is being imported from these modules\n",
    "\n",
    "1. `detection_helpers` which I made to wrap the original `YOLOv-7` code along with helper functions\n",
    "2. `tracking_helpers` has modular code which is used to wrap the `DeepSORT` repo and workings\n",
    "3. `bridge_wrapper` acts as a bridge to bind **ANY** detection model with `DeepSORT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "寛度、高度分别为： 1280 720\n",
      "總幀數： 2472\n",
      "幀速率： 29\n",
      "影片時長約： 1.375 分鐘\n",
      "設定幀速率為： 24\n",
      "共截取圖片： 101  張\n",
      "start\n",
      "24.jpg \n",
      "48.jpg \n",
      "72.jpg \n",
      "96.jpg \n",
      "120.jpg \n",
      "144.jpg \n",
      "168.jpg \n",
      "216.jpg \n",
      "240.jpg \n",
      "264.jpg \n",
      "312.jpg \n",
      "336.jpg \n",
      "432.jpg \n",
      "456.jpg \n",
      "768.jpg \n",
      "792.jpg \n",
      "816.jpg \n",
      "840.jpg \n",
      "864.jpg \n",
      "888.jpg \n",
      "912.jpg \n",
      "936.jpg \n",
      "960.jpg \n",
      "984.jpg \n",
      "1008.jpg \n",
      "1032.jpg \n",
      "1056.jpg \n",
      "1584.jpg \n",
      "1608.jpg \n",
      "1632.jpg \n",
      "1728.jpg \n",
      "1776.jpg \n",
      "1848.jpg \n",
      "1872.jpg \n",
      "1896.jpg \n",
      "2448.jpg \n",
      "end!\n",
      "-------------------------------\n",
      "已保留 36 張圖片,刪除 66 張圖片\n",
      "-------------------------------\n",
      "圖片比對平均用時： 0:00:00.329123\n",
      "總耗時: 0:00:35.753924 \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"跳幀後再讀取,測試視頻使用了36秒\"\"\"\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np \n",
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "VidoePath = './video/lift.mp4'\n",
    "FrameFrequency = 24 # 每frameFrequency保存一張圖片\n",
    "outPutDirName = './image_data/' # 設置保存路徑\n",
    "Similarity = 0.95\n",
    "\n",
    "#cap = cv2.VideoCapture(0, cv2.CAP_DSHOW) # 攝像頭截取\n",
    "cap = cv2.VideoCapture(VidoePath)\n",
    "ret = cap.isOpened # 判斷是否成功打開\n",
    "#print(ret)\n",
    "# 獲取信息 寛高\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # w\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # h\n",
    "print('寛度、高度分别为：',width,height) # 幀率 寛 高\n",
    "n_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print('總幀數：',n_frame) # 整个視頻的總幀數\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 幀率 每秒展示多少張圖片\n",
    "print('幀速率：',int(fps))\n",
    "duration = n_frame/fps/60\n",
    "print(f'影片時長約： {round(duration,3)} 分鐘')\n",
    "    \n",
    "print(f'設定幀速率為： {FrameFrequency}')\n",
    "print(f'共截取圖片： {int((n_frame/FrameFrequency)-2)}  張') \n",
    "\n",
    "\n",
    "k = 0 # 多於兩張圖片時運行\n",
    "count_in=[]\n",
    "count_out=[]\n",
    "img_files=[]\n",
    "spendtime=[]\n",
    "\n",
    "start=datetime.datetime.now()\n",
    "print('start')\n",
    "while ret: \n",
    "    now_fps = cap.get(1)  \n",
    "    ret = cap.grab()\n",
    "    if int(now_fps) == n_frame: # 结束标志是否读取到最后一幀\n",
    "        break        \n",
    "    if (now_fps % 24 == 0):\n",
    "        #print(now_fps)\n",
    "        (flag,frame) = cap.read()\n",
    "        fileName = str(int(now_fps))+'.jpg'  \n",
    "        img_files.append(frame)      \n",
    "        k = k + 1\n",
    "        if k > 1:\n",
    "            img=img_files[-2]\n",
    "            img1=img_files[-1]\n",
    "            start2=datetime.datetime.now()\n",
    "            ssim=structural_similarity(img, img1, channel_axis=-1,win_size=7)#channel_axis=-1,win_size=7\n",
    "            end2=datetime.datetime.now()\n",
    "            spendtime.append(end2-start2)\n",
    "            #print(f'Running time: {end2-start2} ')\n",
    "            if ssim < Similarity:\n",
    "                cv2.imwrite(outPutDirName + fileName, img, [cv2.IMWRITE_JPEG_QUALITY,100])\n",
    "                print(f'{fileName} ')\n",
    "                count_in.append(fileName)\n",
    "            else:\n",
    "                count_out.append(fileName)\n",
    "end=datetime.datetime.now()\n",
    "\n",
    "print('end!')\n",
    "print('-------------------------------')\n",
    "print(f'已保留 {len(count_in)} 張圖片,刪除 {len(count_out)} 張圖片')\n",
    "print('-------------------------------')\n",
    "print(f'圖片比對平均用時： {spendtime[-1]}')\n",
    "print(f'總耗時: {(end-start)} ' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classes = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "    'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "    'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "    'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "    'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "    'hair drier', 'toothbrush']\n",
    "for i ,j in enumerate(classes):\n",
    "    print(i,j)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 person\n",
    "1 bicycle\n",
    "2 car\n",
    "3 motorcycle\n",
    "4 airplane\n",
    "5 bus\n",
    "6 train\n",
    "7 truck\n",
    "8 boat\n",
    "9 traffic light\n",
    "10 fire hydrant\n",
    "11 stop sign\n",
    "12 parking meter\n",
    "13 bench\n",
    "14 bird\n",
    "15 cat\n",
    "16 dog\n",
    "17 horse\n",
    "18 sheep\n",
    "19 cow\n",
    "20 elephant\n",
    "21 bear\n",
    "22 zebra\n",
    "23 giraffe\n",
    "24 backpack\n",
    "25 umbrella\n",
    "26 handbag\n",
    "27 tie\n",
    "28 suitcase\n",
    "29 frisbee\n",
    "30 skis\n",
    "31 snowboard\n",
    "32 sports ball\n",
    "33 kite\n",
    "34 baseball bat\n",
    "35 baseball glove\n",
    "36 skateboard\n",
    "37 surfboard\n",
    "38 tennis racket\n",
    "39 bottle\n",
    "40 wine glass\n",
    "41 cup\n",
    "42 fork\n",
    "43 knife\n",
    "44 spoon\n",
    "45 bowl\n",
    "46 banana\n",
    "47 apple\n",
    "48 sandwich\n",
    "49 orange\n",
    "50 broccoli\n",
    "51 carrot\n",
    "52 hot dog\n",
    "53 pizza\n",
    "54 donut\n",
    "55 cake\n",
    "56 chair\n",
    "57 couch\n",
    "58 potted plant\n",
    "59 bed\n",
    "60 dining table\n",
    "61 toilet\n",
    "62 tv\n",
    "63 laptop\n",
    "64 mouse\n",
    "65 remote\n",
    "66 keyboard\n",
    "67 cell phone\n",
    "68 microwave\n",
    "69 oven\n",
    "70 toaster\n",
    "71 sink\n",
    "72 refrigerator\n",
    "73 book\n",
    "74 clock\n",
    "75 vase\n",
    "76 scissors\n",
    "77 teddy bear\n",
    "78 hair drier\n",
    "79 toothbrush"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\yolo7\\lib\\site-packages\\torch\\nn\\modules\\module.py:831: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  if param.grad is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from detection_helpers import *\n",
    "from tracking_helpers import *\n",
    "from bridge_wrapper import *\n",
    "from PIL import Image\n",
    "\n",
    "detector = Detector(classes = [0]) # it'll detect ONLY [person,horses,sports ball]. class = None means detect all classes. List info at: \"data/coco.yaml\"\n",
    "detector.load_model('./weights/yolov7x.pt',) # pass the path to the trained weight file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./image_data/24.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'24.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" \n",
    "loadpath=r'./image_data/'\n",
    "savepath=r'./image_data_result/'\n",
    "\n",
    "load_img_files=[]\n",
    "filenamelist=[]\n",
    "for rootdir, _, files in os.walk(loadpath):\n",
    "    files.sort(key=lambda x:int(x[:-4]))\n",
    "    #print(files)  \n",
    "for file in files:\n",
    "    if file.endswith('.jpg'):\n",
    "        load_img_files.append(os.path.join(rootdir, file))\n",
    "        filenamelist.append(file)\n",
    "        \n",
    "load_img_files[0]\n",
    "filenamelist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\envs\\yolo7\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./image_data_result/24.jpg\n",
      "./image_data_result/48.jpg\n",
      "./image_data_result/72.jpg\n",
      "./image_data_result/96.jpg\n",
      "./image_data_result/120.jpg\n",
      "./image_data_result/144.jpg\n",
      "./image_data_result/168.jpg\n",
      "./image_data_result/216.jpg\n",
      "./image_data_result/240.jpg\n",
      "./image_data_result/264.jpg\n",
      "./image_data_result/312.jpg\n",
      "./image_data_result/336.jpg\n",
      "./image_data_result/432.jpg\n",
      "./image_data_result/456.jpg\n",
      "./image_data_result/768.jpg\n",
      "./image_data_result/792.jpg\n",
      "./image_data_result/816.jpg\n",
      "./image_data_result/840.jpg\n",
      "./image_data_result/864.jpg\n",
      "./image_data_result/888.jpg\n",
      "./image_data_result/912.jpg\n",
      "./image_data_result/936.jpg\n",
      "./image_data_result/960.jpg\n",
      "./image_data_result/984.jpg\n",
      "./image_data_result/1008.jpg\n",
      "./image_data_result/1032.jpg\n",
      "./image_data_result/1056.jpg\n",
      "./image_data_result/1584.jpg\n",
      "./image_data_result/1608.jpg\n",
      "./image_data_result/1632.jpg\n",
      "./image_data_result/1728.jpg\n",
      "./image_data_result/1776.jpg\n",
      "./image_data_result/1848.jpg\n",
      "./image_data_result/1872.jpg\n",
      "./image_data_result/1896.jpg\n",
      "./image_data_result/2448.jpg\n"
     ]
    }
   ],
   "source": [
    "for currIndex, filename in enumerate(filenamelist):\n",
    "    result = detector.detect(loadpath+filename, plot_bb = True) #, plot_bb = True# plot_bb = False output the predictions as [x,y,w,h, confidence, class]\n",
    "    if len(result.shape) == 3:# If it is image, convert it to proper image. detector will give \"BGR\" image\n",
    "        result = Image.fromarray(cv2.cvtColor(result,cv2.COLOR_BGR2RGB)) \n",
    "        #filename=savepath+str(currIndex)+'.jpg'\n",
    "        result.save(f'{savepath+filename}')   \n",
    "        print(f'{savepath+filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepSORT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb8ede6e48f2442e28c0595893d2b9fa1cb763a73b1e5669a97c4ab5fc34a251"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
